{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIブーム分析.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO51MgmwkJ0OvxLCjNY+Bmm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taniokah/DL-Basic-Seminar/blob/master/AI%E3%83%96%E3%83%BC%E3%83%A0%E5%88%86%E6%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bXkqkqAhMmh",
        "colab_type": "text"
      },
      "source": [
        "#AIブーム分析\n",
        "\n",
        "https://ohke.hateblo.jp/entry/2019/06/01/120000\n",
        "https://qiita.com/a2kiti/items/a93d762bd89a84316c12\n",
        "\n",
        "1. まず、Sudachiのword2vecモデルをダウンロードする\n",
        "2. 対象となる新聞記事を読み込む\n",
        "3. 新聞記事からAIに近い単語を上位10個ずつ抽出する\n",
        "4. 単語ベクトルとその頻度のリストを作る\n",
        "5. 時系列トピックモデルを用いて分析する\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPhrofy5f4XO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "8bebb53e-6a1b-4392-cc26-59df9d06267c"
      },
      "source": [
        "!wget https://object-storage.tyo2.conoha.io/v1/nc_2520839e1f9641b08211a5c85243124a/word_vector/nwjc.sudachi_full_abc_w2v.20190314.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-29 23:58:02--  https://object-storage.tyo2.conoha.io/v1/nc_2520839e1f9641b08211a5c85243124a/word_vector/nwjc.sudachi_full_abc_w2v.20190314.tar.gz\n",
            "Resolving object-storage.tyo2.conoha.io (object-storage.tyo2.conoha.io)... 157.7.224.17\n",
            "Connecting to object-storage.tyo2.conoha.io (object-storage.tyo2.conoha.io)|157.7.224.17|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5212195768 (4.9G) [application/x-www-form-urlencoded]\n",
            "Saving to: ‘nwjc.sudachi_full_abc_w2v.20190314.tar.gz’\n",
            "\n",
            "wjc.sudachi_full_ab  20%[===>                ]   1.01G  74.7MB/s    eta 54s    "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vQfLut0gCk4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d77abd1e-29e8-43f5-d24e-6b69e3252d0f"
      },
      "source": [
        "!tar -zxvf ./nwjc.sudachi_full_abc_w2v.20190314.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nwjc_sudachi_full_abc_w2v/\n",
            "nwjc_sudachi_full_abc_w2v/nwjc.sudachi_full_abc_w2v.txt\n",
            "nwjc_sudachi_full_abc_w2v/LICENSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh5yAlj-gPCF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d40e7e15-bfb9-40a0-af95-f88b182823da"
      },
      "source": [
        "!ls -l nwjc_sudachi_full_abc_w2v/"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 12229548\n",
            "-rw-rw-r-- 1 1008 1008       11420 Mar 20  2019 LICENSE\n",
            "-rw-rw-r-- 1 1008 1008 12523039651 Mar 20  2019 nwjc.sudachi_full_abc_w2v.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY_mN-IshE7n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "57b112db-73e0-4b17-a062-8ac765696b41"
      },
      "source": [
        "!head -n2 nwjc_sudachi_full_abc_w2v/nwjc.sudachi_full_abc_w2v.txt"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3644628 300\n",
            "の -0.08509623 -0.02170456 -0.00595318 0.17987297 -0.02470185 -0.06039717 0.16489118 0.18576832 -0.13870764 -0.17454691 0.08653757 -0.11530939 -0.0357993 0.0308698 -0.11082912 0.04128339 0.00738431 0.00342017 -0.00673456 0.03348965 -0.18876095 0.05184764 0.13508447 0.24196455 -0.07351201 -0.07903086 -0.01973874 -0.24246444 0.03141621 -0.11729659 -0.04126292 -0.20383213 -0.03109288 0.06765312 -0.15020953 -0.11498244 0.03988069 0.20287995 -0.14308368 -0.05135961 -0.15592268 0.05364651 -0.11668575 0.0505892 -0.04575419 0.03296537 -0.03033004 -0.0481239 0.13578212 -0.03111291 -0.15257519 -0.10075521 -0.07440718 -0.12524064 -0.14464405 -0.00155862 0.11825287 0.01658496 0.27135071 -0.07724451 0.01130517 0.03584538 0.01659033 0.19819288 0.01558947 0.07239497 -0.05149494 0.08688559 -0.03712051 -0.07294089 -0.00820937 -0.00907754 -0.03328617 -0.09273465 0.07537545 -0.05137397 0.07578564 -0.03124459 0.05484971 -0.07136369 -0.02457538 0.15481928 0.11072636 -0.00225826 -0.04312619 -0.09835435 0.10180111 -0.11248083 -0.01831779 -0.15895568 0.06585996 0.06583007 -0.02607597 0.07505219 -0.08154492 0.08415596 -0.29794207 0.22361974 0.03844924 -0.02409133 -0.0380278 0.05453932 -0.10982726 0.10995556 0.11009834 0.04420403 -0.05088469 0.06004704 0.01410721 0.10482296 -0.34675267 0.11893709 0.08400187 0.17349045 0.07026103 0.02319975 -0.07899326 -0.09765992 0.03738321 0.10687834 -0.13417427 0.15635394 0.07292477 0.09719512 -0.04300312 0.03135416 0.25681239 0.07999622 -0.10297124 -0.0881549 0.065386 -0.04746811 0.02169232 0.17451568 -0.06795947 -0.30644616 -0.06507952 0.13956924 0.18511888 -0.1142631 0.04402548 0.03508638 -0.12334315 0.00233285 -0.14110087 -0.26152349 -0.12575777 0.10243788 0.03307959 0.08372588 0.2012478 0.02123618 0.0598808 -0.12974231 0.01239459 -0.0255327 -0.11687197 -0.19145888 -0.10097241 0.01828799 0.16954349 -0.18916406 -0.02960817 0.16373363 -0.08409271 0.10939084 -0.10946602 -0.10229198 0.00907564 0.16076608 -0.19279352 0.00508998 -0.0759033 0.01311471 0.1166984 0.24017371 0.06020693 0.00122038 -0.17049754 -0.01034435 -0.09591749 0.2055098 -0.07516637 -0.10714629 -0.17098667 -0.02943251 0.05814891 0.06983829 0.00748073 -0.06156648 0.05453764 -0.07840106 0.1749652 0.25106817 -0.06903429 -0.27245611 0.00350698 0.14503795 -0.06646442 0.0329436 -0.02063798 0.04743206 0.18886574 0.02028349 -0.10190877 -0.18188755 0.27589551 -0.00080682 0.0066181 -0.08410311 -0.25576827 0.07407854 -0.05438402 0.18853284 0.08088059 0.08862372 -0.0970456 0.03529922 0.05947638 0.09120731 0.1907116 0.03028401 -0.06710273 -0.11471934 -0.13359009 0.00544884 -0.01073137 -0.01462306 -0.08237646 0.02719796 0.02005923 -0.11376375 0.18927906 0.0539184 0.11428301 -0.11349536 0.13334441 0.01524157 -0.00129609 0.12784146 -0.06973457 0.01259846 -0.1185156 0.14096615 0.07976481 -0.06855894 0.06703436 -0.15839098 0.07836515 0.05997906 -0.01760146 -0.04950007 0.11396813 0.03565592 0.04952743 0.00099573 0.0437855 0.01601827 0.15287897 -0.08865725 0.04313564 0.13215388 0.01210132 -0.05744287 0.04421859 0.2098541 -0.19167227 -0.16390941 -0.00657649 -0.07565701 -0.13576873 -0.11283566 -0.13851033 0.05974858 -0.16010183 -0.01187299 0.08090579 0.13298063 0.03606028 0.25641844 0.07958484 -0.09526003 0.08859847 -0.1579143 0.09659959 -0.02905053 0.00727161 -0.02406846 -0.03045797 -0.02122181 0.02056854 0.09185477 -0.10587828 -0.06060067 0.07496267 -0.14777204 0.10103506 0.01632762 -0.03348994 0.17903616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMUauoB2iY3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOeM-Xsuh44b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# （今回は使わない）\n",
        "\n",
        "# モデルのロード\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import datapath\n",
        "\n",
        "nwjc_model = KeyedVectors.load_word2vec_format('/content/nwjc_sudachi_full_abc_w2v/nwjc.sudachi_full_abc_w2v.txt', binary=False)\n",
        "\n",
        "#nwjc_model = KeyedVectors.load_word2vec_format(\n",
        "#    datapath('/content/nwjc_sudachi_full_abc_w2v/nwjc.sudachi_full_abc_w2v.txt'),\n",
        "#    binary=False\n",
        "#)\n",
        "\n",
        "# 語数, 次元数\n",
        "print(len(nwjc_model.vocab), nwjc_model.vector_size)  # 3644628 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3vnxWCZorA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# （今回は使わない）\n",
        "\n",
        "print(nwjc_model.most_similar('平成', topn=5))\n",
        "# [('年度', 0.8092531561851501),\n",
        "#  ('平成元年', 0.7409119009971619),\n",
        "#  ('同年', 0.6042426824569702),\n",
        "#  ('基本計画', 0.5917632579803467),\n",
        "#  ('別紙', 0.590812087059021)]\n",
        "\n",
        "print(nwjc_model.most_similar(positive=['兄弟', '女'], negative=['男'], topn=5))\n",
        "# [('姉妹', 0.7687463164329529),\n",
        "#  ('兄妹', 0.7484474182128906),\n",
        "#  ('姉弟', 0.715774416923523),\n",
        "#  ('双子', 0.642593264579773),\n",
        "#  ('妹', 0.6402267217636108)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scJPEOW4owN9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "d4167d28-4c87-42a9-9a0f-167561b9e4d6"
      },
      "source": [
        "!pip install SudachiPy"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting SudachiPy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/c9/40bfb291a7995ad218451ef97083432f998b822e3ecbd9f586f593d2cfb6/SudachiPy-0.4.2-py3-none-any.whl (73kB)\n",
            "\r\u001b[K     |████▌                           | 10kB 17.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 30kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 61kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 71kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers~=2.1.0 in /usr/local/lib/python3.6/dist-packages (from SudachiPy) (2.1.0)\n",
            "Collecting dartsclone~=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/4d/45acbe9d0795d8ceef0fee1f9ac2dcbf27dca3a0578a023fcdc3fef6fd89/dartsclone-0.6.tar.gz\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from dartsclone~=0.6.0->SudachiPy) (0.29.14)\n",
            "Building wheels for collected packages: dartsclone\n",
            "  Building wheel for dartsclone (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dartsclone: filename=dartsclone-0.6-cp36-cp36m-linux_x86_64.whl size=413256 sha256=0c6dc0f632957a8dc4d5f55d8816d2e7cbfeb2657d56bb55b0735a9d52ad6d37\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/cd/70/fe43307bf7398243155108f4f5a258ef336923d65ec4af93cd\n",
            "Successfully built dartsclone\n",
            "Installing collected packages: dartsclone, SudachiPy\n",
            "Successfully installed SudachiPy-0.4.2 dartsclone-0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0y6f7mtdC1QJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "5d3692c9-0ff5-47b2-bc39-0211f499b7d3"
      },
      "source": [
        "!pip install https://object-storage.tyo2.conoha.io/v1/nc_2520839e1f9641b08211a5c85243124a/sudachi/SudachiDict_core-20191224.tar.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://object-storage.tyo2.conoha.io/v1/nc_2520839e1f9641b08211a5c85243124a/sudachi/SudachiDict_core-20191224.tar.gz\n",
            "\u001b[?25l  Downloading https://object-storage.tyo2.conoha.io/v1/nc_2520839e1f9641b08211a5c85243124a/sudachi/SudachiDict_core-20191224.tar.gz (70.7MB)\n",
            "\u001b[K     |████████████████████████████████| 70.7MB 61kB/s \n",
            "\u001b[?25hRequirement already satisfied: SudachiPy>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from SudachiDict-core==20191224) (0.4.2)\n",
            "Requirement already satisfied: sortedcontainers~=2.1.0 in /usr/local/lib/python3.6/dist-packages (from SudachiPy>=0.2.1->SudachiDict-core==20191224) (2.1.0)\n",
            "Requirement already satisfied: dartsclone~=0.6.0 in /usr/local/lib/python3.6/dist-packages (from SudachiPy>=0.2.1->SudachiDict-core==20191224) (0.6)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from dartsclone~=0.6.0->SudachiPy>=0.2.1->SudachiDict-core==20191224) (0.29.14)\n",
            "Building wheels for collected packages: SudachiDict-core\n",
            "  Building wheel for SudachiDict-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for SudachiDict-core: filename=SudachiDict_core-20191224-cp36-none-any.whl size=70876855 sha256=13d033b6c96ae617b096144f2c47f1cfa957974baa6e23fd4fd79eeb04e5bfa5\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/88/5c/44c0deb66c147b881dab3fe061febbb301bdceab7e9b496479\n",
            "Successfully built SudachiDict-core\n",
            "Installing collected packages: SudachiDict-core\n",
            "Successfully installed SudachiDict-core-20191224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdAgLPT5Dgt0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "720772b7-631e-4efd-de2c-d1194285aa5b"
      },
      "source": [
        "!sudachipy tokenize -h"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: sudachipy tokenize [-h] [-r file] [-m {A,B,C}] [-o file] [-a] [-d] [-v]\n",
            "                          [file [file ...]]\n",
            "\n",
            "Tokenize Text\n",
            "\n",
            "positional arguments:\n",
            "  file           text written in utf-8\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help     show this help message and exit\n",
            "  -r file        the setting file in JSON format\n",
            "  -m {A,B,C}     the mode of splitting\n",
            "  -o file        the output file\n",
            "  -a             print all of the fields\n",
            "  -d             print the debug information\n",
            "  -v, --version  print sudachipy version\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phm-ex6po4rN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "fbb3b6e2-aa1a-42c9-f24d-0f23c624bae9"
      },
      "source": [
        "!pip install mojimoji"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mojimoji\n",
            "  Downloading https://files.pythonhosted.org/packages/10/2d/f5d58e74af1c1b4eeea66ecc14f35ae8f275f39bc7158ba51a6e8a86dfdf/mojimoji-0.0.9.post0.tar.gz\n",
            "Building wheels for collected packages: mojimoji\n",
            "  Building wheel for mojimoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mojimoji: filename=mojimoji-0.0.9-cp36-cp36m-linux_x86_64.whl size=117567 sha256=12c8895291a04dbef55d4fddde1c8fa8df4253b1a0d87a393d0fd6ea3521a756\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/f0/6e/03e5c5017afc8230e19abeacad57189138207fea2f5dd71880\n",
            "Successfully built mojimoji\n",
            "Installing collected packages: mojimoji\n",
            "Successfully installed mojimoji-0.0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBiBrRDRD3NY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "eee1c7d4-8872-49d3-f7e0-52961164b032"
      },
      "source": [
        "# 前もって sudachiData.pickle をアップロードしておくと起動が速い\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "import csv\n",
        "import os\n",
        "from sudachipy import tokenizer\n",
        "from sudachipy import dictionary\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import mojimoji\n",
        "\n",
        "class Sudachi_w2v:\n",
        "    def __init__(self, path, sudachiDataPath=\"sudachiData.pickle\"):\n",
        "        self.cache = {}\n",
        "        f = open(path, 'r')\n",
        "        self.file = f\n",
        "        self.reader = csv.reader(f, delimiter=' ')\n",
        "        #最初に含有単語リストやメモリアドレスリストを作成する（かなり時間かかる）\n",
        "        #2回目以降はpickle化したものを読み込む\n",
        "        if os.path.exists(sudachiDataPath):\n",
        "            with open(sudachiDataPath, 'rb') as f:\n",
        "                dataset = pickle.load(f)\n",
        "            self.offset_list = dataset[\"offset_list\"]\n",
        "            self.emb_size = dataset[\"emb_size\"]\n",
        "            self.word2index = dataset[\"word2index\"]\n",
        "            self.ave_vec = dataset[\"ave_vec\"]\n",
        "        else:\n",
        "            txt = f.readline()\n",
        "            #分散表現の次元数\n",
        "            self.emb_size = int(txt.split()[1])\n",
        "            #未知語が来た場合平均ベクトルを返す\n",
        "            self.ave_vec = np.zeros(self.emb_size, np.float)\n",
        "            #メモリアドレスリスト\n",
        "            self.offset_list = []\n",
        "            word_list = []\n",
        "            count = 0\n",
        "            maxCount = int(txt.split()[0])\n",
        "            while True:\n",
        "                count+=1\n",
        "                self.offset_list.append(f.tell())\n",
        "                if count % 100000 == 0:print(count,\"/\",maxCount)\n",
        "                line = f.readline()\n",
        "                if line == '':break\n",
        "                line_list = line.split()\n",
        "                word_list.append(line_list[0])\n",
        "                self.ave_vec += np.array(line_list[-300:]).astype(np.float)\n",
        "            self.offset_list.pop()\n",
        "            self.ave_vec = self.ave_vec/count\n",
        "            self.word2index = {v:k for k,v in enumerate(word_list)}\n",
        "\n",
        "            dataset = {}\n",
        "            dataset[\"offset_list\"] = self.offset_list\n",
        "            dataset[\"emb_size\"] = self.emb_size\n",
        "            dataset[\"word2index\"] = self.word2index\n",
        "            dataset[\"ave_vec\"] = self.ave_vec\n",
        "            with open(sudachiDataPath, 'wb') as f:\n",
        "                pickle.dump(dataset, f)\n",
        "\n",
        "        self.num_rows = len(self.offset_list)\n",
        "        #sudachiの準備\n",
        "        self.tokenizer_obj = dictionary.Dictionary().create()\n",
        "        self.mode = tokenizer.Tokenizer.SplitMode.C\n",
        "\n",
        "    #単語をベクトル化\n",
        "    def word2vec(self, word):\n",
        "        try:\n",
        "            idx = self.word2index[word]\n",
        "            result = self.read_row(idx)\n",
        "            vec = np.array(result[-300:])\n",
        "            return vec\n",
        "        except:#単語リストにない場合\n",
        "            print(word, \": out of wordlist\")\n",
        "\n",
        "    #文章を分かち書きした後，それぞれのベクトルをmatでまとめて返す\n",
        "    def sentence2mat(self, sentence):\n",
        "        words = sentence.replace(\"　\",\" \").replace(\"\\n\",\" \")\n",
        "        words = re.sub(r\"\\s+\", \" \", words)\n",
        "        input_seq = [mojimoji.zen_to_han(m.surface()) for m in self.tokenizer_obj.tokenize(words, self.mode)]\n",
        "        input_seq = [s.lower() for s in input_seq]\n",
        "        input_seq = [s for s in input_seq if s != ' ']\n",
        "\n",
        "        mat = np.zeros((len(input_seq), self.emb_size))\n",
        "        input_sentence = []\n",
        "        for i, word in enumerate(input_seq):\n",
        "            try:\n",
        "                idx = self.word2index[word]\n",
        "                result = self.read_row(idx)\n",
        "                input_sentence.append(result[0])\n",
        "                mat[i] = np.array(result[-300:])\n",
        "            except:#単語リストにない場合平均ベクトルを返す\n",
        "                input_sentence.append(\"<UNK>\")\n",
        "                mat[i] = self.ave_vec\n",
        "        return input_sentence, mat\n",
        "\n",
        "    def __del__(self):\n",
        "        self.file.close()\n",
        "\n",
        "    def read_row(self, idx):\n",
        "        self.file.seek(self.offset_list[idx])\n",
        "        return next(self.reader)\n",
        "    \n",
        "    def cos(self, vec1, vec2):\n",
        "        key = string(vec1) + string(vec2)\n",
        "        if (key in cache.keys()): \n",
        "          return cache[key]\n",
        "        else \n",
        "          similarity = round(float(cosine_similarity([vec1], [vec2])[0][0]), 6)\n",
        "          cache[key] = similarity\n",
        "          return similarity"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-99-eefd57daf6a2>\"\u001b[0;36m, line \u001b[0;32m106\u001b[0m\n\u001b[0;31m    else\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s40xdU-fNjd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/nwjc_sudachi_full_abc_w2v/nwjc.sudachi_full_abc_w2v.txt'\n",
        "sudachi = Sudachi_w2v(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIaiRyQJN1dF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vec = sudachi.word2vec(\"すだち\")\n",
        "print(vec)\n",
        "#['0.07975651' '0.08931299' '-0.06070593' '0.46959993' '0.19651023' ~\n",
        "\n",
        "input_sentence, mat = sudachi.sentence2mat(\"あきらめたらそこで試合終了だよ\")\n",
        "print(input_sentence)\n",
        "print(mat)\n",
        "#(['あきらめ', 'たら', 'そこ', 'で', '試合終了', 'だ', 'よ'], array([[ 1.9788130e-02,  1.1190426e-01, -1.6153505e-01, ...,\n",
        "\n",
        "input_sentence, mat = sudachi.sentence2mat(\"人工知能（ＡＩ）の進化を受け、データの活用法は企業の競争力を左右する。\")\n",
        "print(input_sentence)\n",
        "print(mat)\n",
        "\n",
        "input_sentence, mat = sudachi.sentence2mat(\"AIってなんだろ？\")\n",
        "print(input_sentence)\n",
        "print(mat)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdOBZzzDqSSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 新聞記事をNK以下にアップロードする\n",
        "\n",
        "#!mkdir NK\n",
        "#!mv NKM*.csv NK/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF26cHWPSdTM",
        "colab_type": "text"
      },
      "source": [
        "## 新聞記事を読み込む"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsd5wNLgRDgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "\n",
        "with open(\"/content/NK/NKM_2018.csv\", newline=\"\") as f:\n",
        "  reader = csv.reader(f, delimiter=\",\", quotechar='\"')\n",
        "\n",
        "  count = 0\n",
        "  label = []\n",
        "  for row in reader:\n",
        "    count += 1\n",
        "    if count == 1:\n",
        "      label.append(row[9])\n",
        "      label.append(row[10])\n",
        "      continue\n",
        "    elif count >= 10:\n",
        "      break\n",
        "    print(label[0], row[9])\n",
        "    print(label[1], row[10])\n",
        "    print('\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qB3c3dOYkgj6",
        "colab": {}
      },
      "source": [
        "# SudachiPyで解析してmatを返す\n",
        "\n",
        "with open(\"/content/NK/NKM_2018.csv\", newline=\"\") as f:\n",
        "  reader = csv.reader(f, delimiter=\",\", quotechar='\"')\n",
        "\n",
        "  count = 0\n",
        "  label = []\n",
        "  for row in reader:\n",
        "    count += 1\n",
        "    if count == 1:\n",
        "      label.append(row[9])\n",
        "      label.append(row[10])\n",
        "      continue\n",
        "    elif count >= 10:\n",
        "      break\n",
        "    input_sentence, mat = sudachi.sentence2mat(row[9])\n",
        "    print(label[0], input_sentence)\n",
        "    #print(mat)\n",
        "    input_sentence, mat = sudachi.sentence2mat(row[10])\n",
        "    print(label[1], input_sentence)\n",
        "    #print(mat)\n",
        "    print('\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur7DcuYs0Rtr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "86cccfe8-f29f-456e-ca8b-19a6aa565913"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity = round(float(cosine_similarity([[1, 0, -1]], [[-1,-1, 0]])[0][0]), 6)\n",
        "print(similarity)\n",
        "\n",
        "similarity = round(float(cosine_similarity([[-0.08247425,  0.17477778,  0.08027521,  0.32425445,  0.17314065,  -0.10243454]], \n",
        "                                     [[-0.04262996,  0.08501831,  0.01364794,  0.00316689,  0.04561372,  0.30977321]])[0][0]), 6)\n",
        "print(similarity)\n",
        "\n",
        "similarity = round(float(cosine_similarity([[-0.02831401,  0.10125325, -0.12704873, 0.35899675,  0.03712214,   0.02951937]], \n",
        "                                     [[-0.04262996,  0.08501831,  0.01364794,  0.00316689,  0.04561372,  0.30977321]])[0][0]), 6)\n",
        "print(similarity)\n",
        "\n",
        "similarity = round(float(cosine_similarity([[-0.04262996,  0.08501831,  0.01364794,  0.00316689,  0.04561372,  0.30977321]], \n",
        "                                     [[-0.04262996,  0.08501831,  0.01364794,  0.00316689,  0.04561372,  0.30977321]])[0][0]), 5)\n",
        "print(similarity)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.5\n",
            "-0.023404\n",
            "0.153889\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNnbv2OLPFEX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "df9a6483-c0ad-4e64-8220-5bb3050d5da1"
      },
      "source": [
        "def cos(word):\n",
        "  vec1 = sudachi.word2vec(word[0])\n",
        "  vec2 = sudachi.word2vec(word[1])\n",
        "  sim = sudachi.cos(vec1, vec2)\n",
        "  print(\"cos(\" + word[0] + \", \" + word[1] + \") \\n =>\", sim)\n",
        "\n",
        "cos([\"ai\", \"人工知能\"])\n",
        "cos([\"ai\", \"機械学習\"])\n",
        "cos([\"人工知能\", \"機械学習\"])\n",
        "cos([\"人工知能\", \"自然言語処理\"])\n",
        "cos([\"機械学習\", \"自然言語処理\"])\n",
        "cos([\"ai\", \"自然言語処理\"])\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cos(ai, 人工知能) \n",
            " => 0.595735\n",
            "cos(ai, 機械学習) \n",
            " => 0.378244\n",
            "cos(人工知能, 機械学習) \n",
            " => 0.57597\n",
            "cos(人工知能, 自然言語処理) \n",
            " => 0.569241\n",
            "cos(機械学習, 自然言語処理) \n",
            " => 0.641625\n",
            "cos(ai, 自然言語処理) \n",
            " => 0.327691\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUmhbNvdVEtR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SudachiPyで解析して\"AI\"に対するcosを返す\n",
        "\n",
        "import csv\n",
        "\n",
        "def coscount(filedir, fname, query, thres):\n",
        "  ai = sudachi.word2vec(query)\n",
        "\n",
        "  with open(filedir + fname + \"_AI.csv\", 'w', newline='') as fout:\n",
        "      writer = csv.writer(fout, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "      writer.writerow(['date', 'mediacode', 'headbody', 'word', 'cosine'])\n",
        "\n",
        "      total = 0\n",
        "      with open(filedir + fname + \".csv\", newline=\"\") as fin:\n",
        "        reader = csv.reader(fin, delimiter=\",\", quotechar='\"')\n",
        "\n",
        "        count = 0\n",
        "        label = []\n",
        "        for row in reader:\n",
        "          count += 1\n",
        "          if count == 1:\n",
        "            label.append(row[9])\n",
        "            label.append(row[10])\n",
        "            continue\n",
        "          \n",
        "          input_sentence, mat = sudachi.sentence2mat(row[9])\n",
        "          i = 0\n",
        "          for vec in mat:\n",
        "            sim = sudachi.cos(ai, vec)\n",
        "            if (sim > thres):\n",
        "              #print(count, label[0], input_sentence[i], sim) #, vec)\n",
        "              #print([row[1], row[3], label[0], input_sentence[i], sim])\n",
        "              writer.writerow([row[1], row[3], label[0], input_sentence[i], sim])\n",
        "              total += 1\n",
        "            i += 1\n",
        "\n",
        "          input_sentence, mat = sudachi.sentence2mat(row[10])\n",
        "          i = 0\n",
        "          for vec in mat:\n",
        "            sim = sudachi.cos(ai, vec)\n",
        "            if (sim > thres):\n",
        "              #print(count, label[1], input_sentence[i], sim) #, vec)\n",
        "              #print([row[1], row[3], label[1], input_sentence[i], sim])\n",
        "              writer.writerow([row[1], row[3], label[1], input_sentence[i], sim])\n",
        "              total += 1\n",
        "            i += 1\n",
        "\n",
        "  return total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxSnQw5GcEu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "coscount(\"/content/NK/\", \"NKM_2000\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKM_2001\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKM_2002\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKM_2003\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKM_2004\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKM_2005\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKM_2006\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKM_2007\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKM_2008\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKM_2009\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKM_2010\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKM_2011\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKM_2012\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKM_2013\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKM_2014\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKM_2015\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKM_2016\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKM_2017\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKM_2018\", \"人工知能\", 0.5)\n",
        "\n",
        "coscount(\"/content/NK/\", \"NKL_2000\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKL_2001\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKL_2002\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKL_2003\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKL_2004\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKL_2005\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKL_2006\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKL_2007\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKL_2008\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKL_2009\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKL_2010\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKL_2011\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKL_2012\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKL_2013\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKL_2014\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKL_2015\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKL_2016\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKL_2017\", \"人工知能\", 0.5)\n",
        "coscount(\"/content/NK/\", \"NKL_2018\", \"人工知能\", 0.5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCGGGign0hZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"/content/NK/NKM_2000_AI.csv\")\n",
        "files.download(\"/content/NK/NKM_2001_AI.csv\")\n",
        "files.download(\"/content/NK/NKM_2002_AI.csv\")\n",
        "files.download(\"/content/NK/NKM_2003_AI.csv\")\n",
        "files.download(\"/content/NK/NKM_2004_AI.csv\")\n",
        "files.download(\"/content/NK/NKM_2005_AI.csv\")\n",
        "files.download(\"/content/NK/NKM_2006_AI.csv\")\n",
        "files.download(\"/content/NK/NKM_2007_AI.csv\")\n",
        "files.download(\"/content/NK/NKM_2008_AI.csv\")\n",
        "files.download(\"/content/NK/NKM_2009_AI.csv\")\n",
        "files.download(\"/content/NK/NKM_2010_AI.csv\")\n",
        "files.download(\"/content/NK/NKM_2011_AI.csv\")\n",
        "files.download(\"/content/NK/NKM_2012_AI.csv\")\n",
        "files.download(\"/content/NK/NKM_2013_AI.csv\")\n",
        "files.download(\"/content/NK/NKM_2014_AI.csv\")\n",
        "files.download(\"/content/NK/NKM_2015_AI.csv\")\n",
        "files.download(\"/content/NK/NKM_2016_AI.csv\")\n",
        "files.download(\"/content/NK/NKM_2017_AI.csv\")\n",
        "files.download(\"/content/NK/NKM_2018_AI.csv\")\n",
        "\n",
        "files.download(\"/content/NK/NKL_2000_AI.csv\")\n",
        "files.download(\"/content/NK/NKL_2001_AI.csv\")\n",
        "files.download(\"/content/NK/NKL_2002_AI.csv\")\n",
        "files.download(\"/content/NK/NKL_2003_AI.csv\")\n",
        "files.download(\"/content/NK/NKL_2004_AI.csv\")\n",
        "files.download(\"/content/NK/NKL_2005_AI.csv\")\n",
        "files.download(\"/content/NK/NKL_2006_AI.csv\")\n",
        "files.download(\"/content/NK/NKL_2007_AI.csv\")\n",
        "files.download(\"/content/NK/NKL_2008_AI.csv\")\n",
        "files.download(\"/content/NK/NKL_2009_AI.csv\")\n",
        "files.download(\"/content/NK/NKL_2010_AI.csv\")\n",
        "files.download(\"/content/NK/NKL_2011_AI.csv\")\n",
        "files.download(\"/content/NK/NKL_2012_AI.csv\")\n",
        "files.download(\"/content/NK/NKL_2013_AI.csv\")\n",
        "files.download(\"/content/NK/NKL_2014_AI.csv\")\n",
        "files.download(\"/content/NK/NKL_2015_AI.csv\")\n",
        "files.download(\"/content/NK/NKL_2016_AI.csv\")\n",
        "files.download(\"/content/NK/NKL_2017_AI.csv\")\n",
        "files.download(\"/content/NK/NKL_2018_AI.csv\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXtk-Ud4Wt1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}